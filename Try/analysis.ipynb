{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import phi.torch\n",
    "import torch\n",
    "# import phi.torch as math\n",
    "from phi.torch import *\n",
    "\n",
    "class NavigationNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.control_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(6, 16),  # Input: [bot_x, bot_y, goal_x, goal_y, world_theta, relative_theta]\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 64),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(16, 2), # Output: [L, delta_theta]\n",
    "            torch.nn.Tanh()  # Tanh activation for bounded output\n",
    "        )\n",
    "        \n",
    "    def forward(self, current_state):\n",
    "        return self.control_net(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trajectory(net, initial_pos, goal_pos, max_steps=12):\n",
    "\n",
    "    current_pos = initial_pos\n",
    "    theta = math.zeros(initial_pos.shape.non_channel)\n",
    "    total_loss = 0\n",
    "    path_length = 0  # Track total distance traveled\n",
    "    prev_controls = None  # For control smoothness\n",
    "    \n",
    "    # --- New: Store trajectory for final position loss ---\n",
    "    trajectory = [current_pos]\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        delta_pos = goal_pos - current_pos\n",
    "\n",
    "        # Temporal discount factor ---\n",
    "        temporal_weight = 0.85 ** (step*-1)\n",
    "        \n",
    "        # Calculate relative angle using existing vector components\n",
    "        relative_theta = math.arctan(delta_pos.vector['y'], divide_by=delta_pos.vector['x']+eps) - theta\n",
    "\n",
    "        relative_theta = (relative_theta + np.pi) % (2 * np.pi) - np.pi \n",
    "        \n",
    "        # Network input\n",
    "        net_input = math.stack([\n",
    "            current_pos.vector['x']/10, \n",
    "            current_pos.vector['y']/7,\n",
    "            goal_pos.vector['x']/10,\n",
    "            goal_pos.vector['y']/7,\n",
    "            theta/math.PI,\n",
    "            relative_theta/math.PI\n",
    "        ], channel('input_features'))\n",
    "        \n",
    "        # Network prediction\n",
    "        controls = math.native_call(net, net_input)\n",
    "        L = controls.vector[0]\n",
    "        delta_theta = controls.vector[1]*math.PI\n",
    "\n",
    "        if prev_controls is not None:\n",
    "            control_change = math.vec_squared(controls - prev_controls)\n",
    "            total_loss += 0.25 * math.mean(control_change)\n",
    "        prev_controls = controls\n",
    "\n",
    "        # Update orientation with physical constraints\n",
    "        # theta += math.clip(delta_theta, -math.PI/9, math.PI/9)\n",
    "        theta += delta_theta\n",
    "\n",
    "        theta = (theta + np.pi) % (2 * np.pi) - np.pi \n",
    "        \n",
    "        # Calculate movement using existing vector dimension\n",
    "        delta_x = L * math.cos(theta)\n",
    "        delta_y = L * math.sin(theta)\n",
    "\n",
    "        movement = math.stack([delta_x, delta_y], dim=channel(vector='x,y'))\n",
    "\n",
    "        # --- New: Track path length ---\n",
    "        path_length += math.vec_length(movement)\n",
    "        \n",
    "        # Update position\n",
    "        new_pos = current_pos + movement\n",
    "        trajectory.append(new_pos)\n",
    "\n",
    "        # --- Improved: Discounted position loss ---\n",
    "        position_loss = temporal_weight * math.vec_length(delta_pos)\n",
    "        control_loss = 0.1 * (math.abs(delta_theta))\n",
    "        \n",
    "        total_loss += math.mean(position_loss  + control_loss) #\n",
    "        \n",
    "        current_pos = math.where(math.vec_length(delta_pos) > 0.1, new_pos, current_pos)\n",
    "    \n",
    "    final_pos_loss = 10.0 * math.vec_length(trajectory[-1] - goal_pos)\n",
    "    \n",
    "    # --- New: Path efficiency penalty ---\n",
    "    straight_line_dist = math.vec_length(goal_pos - initial_pos)\n",
    "    efficiency_loss = 0.9 * (path_length / (straight_line_dist + eps))  # Prevent div/0\n",
    "    \n",
    "    return total_loss + math.mean(final_pos_loss + efficiency_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi.torch.flow import batch, channel\n",
    "\n",
    "\n",
    "def generate_batch(batch_size, min_distance=2.0, max_distance=12):\n",
    "    batch_dim = batch(samples=batch_size)\n",
    "    vec_dim = channel(vector='x,y')\n",
    "    \n",
    "    # Generate initial positions\n",
    "    initial_x = math.random_uniform(batch_dim, low=-8, high=8)\n",
    "    initial_y = math.random_uniform(batch_dim, low=-6,  high=6)\n",
    "    initial_pos = math.stack([initial_x, initial_y], vec_dim)\n",
    "    \n",
    "    # Generate random displacement direction (angles)\n",
    "    angle = math.random_uniform(batch_dim, low=-math.pi, high=math.pi)\n",
    "    \n",
    "    # Generate displacement magnitudes between [min_distance, 2*min_distance]\n",
    "    distance = math.random_uniform(batch_dim, low=min_distance, high=max_distance)\n",
    "    \n",
    "    # Compute displacement components\n",
    "    dx = distance * math.cos(angle)\n",
    "    dy = distance * math.sin(angle)\n",
    "    \n",
    "    # Apply displacement to initial positions\n",
    "    goal_x = (initial_x + dx)\n",
    "    goal_y = (initial_y + dy)\n",
    "    \n",
    "    # Clamp goals to stay within bounds\n",
    "    goal_x = math.clip(goal_x, -7.9, 7.9)\n",
    "    goal_y = math.clip(goal_y, -5.9, 5.9)\n",
    "    \n",
    "    goal_pos = math.stack([goal_x, goal_y], vec_dim)\n",
    "    \n",
    "    return initial_pos, goal_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss(net, initial_pos, goal_pos):\n",
    "    return simulate_trajectory(net, initial_pos, goal_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_trajectory(net, initial_pos, goal_pos, max_steps=12):\n",
    "    \"\"\"Simulate and plot a single trajectory using PyTorch tensors\"\"\"\n",
    "    with torch.no_grad():\n",
    "        current_pos = initial_pos.clone()\n",
    "        goal_pos = goal_pos.clone()\n",
    "\n",
    "        print(current_pos, goal_pos)\n",
    "        \n",
    "        theta = torch.zeros_like(current_pos[0])\n",
    "        positions = []\n",
    "        \n",
    "        for stp in range(max_steps):\n",
    "            delta_pos = goal_pos - current_pos\n",
    "            relative_theta = torch.atan2(delta_pos[1], delta_pos[0]) - theta\n",
    "\n",
    "            relative_theta = (relative_theta + np.pi) % (2 * np.pi) - np.pi\n",
    "            \n",
    "            net_input = torch.stack([\n",
    "                current_pos[0]/8, current_pos[1]/6,\n",
    "                goal_pos[0]/8, goal_pos[1]/6,\n",
    "                theta/math.PI, relative_theta/math.PI\n",
    "            ], dim=-1).unsqueeze(0)\n",
    "            \n",
    "            controls = net(net_input)[0]\n",
    "            L = controls[0]\n",
    "            delta_theta = controls[1]*math.PI\n",
    "\n",
    "            # print(L, delta_theta, theta, torch.clip(delta_theta, -math.PI/9, math.PI/9))\n",
    "\n",
    "            theta = theta + delta_theta\n",
    "            theta = (theta + np.pi) % (2 * np.pi) - np.pi \n",
    "\n",
    "\n",
    "            # print(\"step\", stp, \" \", np.rad2deg(theta), net_input)\n",
    "        \n",
    "            \n",
    "            # theta += delta_theta\n",
    "            movement = torch.stack([\n",
    "                L * torch.cos(theta),\n",
    "                L * torch.sin(theta)\n",
    "            ])\n",
    "            \n",
    "            current_pos += movement\n",
    "            # Append a copy of the numpy array to avoid reference issues\n",
    "            positions.append(current_pos.cpu().numpy().copy())  # Fixed line\n",
    "            \n",
    "            if torch.norm(delta_pos) < 0.1:\n",
    "                break\n",
    "        \n",
    "        positions = np.array(positions)\n",
    "        # Rest of the plotting code remains the same\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(positions[:, 0], positions[:, 1], 'b-o', markersize=4, label='Path')\n",
    "        plt.scatter(positions[0, 0], positions[0, 1], c='green', s=200, marker='*', label='Start')\n",
    "        plt.scatter(goal_pos[0].item(), goal_pos[1].item(), c='red', s=200, marker='X', label='Goal')\n",
    "        plt.title(\"Navigation Trajectory\")\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.grid(True)\n",
    "        plt.axis('equal')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m initial_pos, goal_pos \u001b[38;5;241m=\u001b[39m generate_batch(\u001b[38;5;241m512\u001b[39m, min_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, max_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mphysics_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoal_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output\n\u001b[0;32m     14\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum  \u001b[38;5;66;03m# Sum the loss to get a scalar\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mphysics_loss\u001b[1;34m(net, initial_pos, goal_pos)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mphysics_loss\u001b[39m(net, initial_pos, goal_pos):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimulate_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoal_pos\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m, in \u001b[0;36msimulate_trajectory\u001b[1;34m(net, initial_pos, goal_pos, max_steps)\u001b[0m\n\u001b[0;32m     27\u001b[0m net_input \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mstack([\n\u001b[0;32m     28\u001b[0m     current_pos\u001b[38;5;241m.\u001b[39mvector[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m     29\u001b[0m     current_pos\u001b[38;5;241m.\u001b[39mvector[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m7\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     relative_theta\u001b[38;5;241m/\u001b[39mmath\u001b[38;5;241m.\u001b[39mPI\n\u001b[0;32m     34\u001b[0m ], channel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Network prediction\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m controls \u001b[38;5;241m=\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnative_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m L \u001b[38;5;241m=\u001b[39m controls\u001b[38;5;241m.\u001b[39mvector[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m delta_theta \u001b[38;5;241m=\u001b[39m controls\u001b[38;5;241m.\u001b[39mvector[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mmath\u001b[38;5;241m.\u001b[39mPI\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\phiml\\math\\_ops.py:193\u001b[0m, in \u001b[0;36mnative_call\u001b[1;34m(f, channels_last, channel_dim, spatial_dim, *inputs)\u001b[0m\n\u001b[0;32m    191\u001b[0m     groups \u001b[38;5;241m=\u001b[39m [b_dims, \u001b[38;5;241m*\u001b[39mi\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mspatial\u001b[38;5;241m.\u001b[39mnames, i\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mchannel] \u001b[38;5;28;01mif\u001b[39;00m channels_last \u001b[38;5;28;01melse\u001b[39;00m [b_dims, i\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;241m*\u001b[39mi\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mspatial\u001b[38;5;241m.\u001b[39mnames]\n\u001b[0;32m    192\u001b[0m     natives\u001b[38;5;241m.\u001b[39mappend(i\u001b[38;5;241m.\u001b[39mnative(groups, force_expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m--> 193\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnatives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(channel_dim, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    195\u001b[0m     channel_dim \u001b[38;5;241m=\u001b[39m channel(channel_dim)\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mNavigationNet.forward\u001b[1;34m(self, current_state)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_state):\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import phiml.math as math\n",
    "\n",
    "net = NavigationNet()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    # Generate batch using PhiFlow\n",
    "    initial_pos, goal_pos = generate_batch(512, min_distance=2.0, max_distance=10.0)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = physics_loss(net, initial_pos, goal_pos)\n",
    "    loss = output[0] if isinstance(output, tuple) else output\n",
    "    total_loss = loss.sum  # Sum the loss to get a scalar\n",
    "\n",
    "    # Skip backward and step if loss is NaN/Inf\n",
    "    if not torch.isfinite(total_loss):\n",
    "        print(f\"Epoch {epoch}: Loss is NaN/Inf, skipping update.\")\n",
    "        continue\n",
    "\n",
    "    total_loss.backward()  # Backpropagate\n",
    "\n",
    "    # Clip gradients to prevent explosion (adjust max_norm as needed)\n",
    "    torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "\n",
    "    # Check for NaNs in gradients\n",
    "    has_nan = False\n",
    "    for p in net.parameters():\n",
    "        if p.grad is not None and not torch.all(torch.isfinite(p.grad)):\n",
    "            has_nan = True\n",
    "            break\n",
    "\n",
    "    if has_nan:\n",
    "        print(f\"Epoch {epoch}: NaN detected in gradients, skipping step.\")\n",
    "        optimizer.zero_grad()  # Clear gradients to prevent contamination\n",
    "    else:\n",
    "        optimizer.step()  # Update parameters if no NaNs\n",
    "\n",
    "    # Logging and plotting\n",
    "    if epoch % 500 == 0:\n",
    "        with torch.no_grad():\n",
    "            initial_pos, goal_pos = generate_batch(1, 4)\n",
    "            initial_torch = initial_pos.native(\"samples,vector\").squeeze(0)\n",
    "            goal_torch = goal_pos.native(\"samples,vector\").squeeze(0)\n",
    "\n",
    "            loss = physics_loss(net, initial_pos, goal_pos)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.native().item():.4f}\")\n",
    "\n",
    "            plot_trajectory(net, initial_torch, goal_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initial_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m initial_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m4.5\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3.2\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      3\u001b[0m goal_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2.1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6.1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 5\u001b[0m loss \u001b[38;5;241m=\u001b[39m physics_loss(net, \u001b[43minitial_pos\u001b[49m, goal_pos)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(f\"Epoch {epoch}, Loss: {loss.native().item():.4f}\")\u001b[39;00m\n\u001b[0;32m      8\u001b[0m plot_trajectory(net, initial_torch, goal_torch, \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initial_pos' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    initial_torch = torch.tensor([4.5, -3.2], dtype=torch.float32)\n",
    "    goal_torch = torch.tensor([2.1, -6.1], dtype=torch.float32)\n",
    "\n",
    "    loss = physics_loss(net, initial_torch, goal_torch)\n",
    "    # print(f\"Epoch {epoch}, Loss: {loss.native().item():.4f}\")\n",
    "\n",
    "    plot_trajectory(net, initial_torch, goal_torch, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'nav_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zf/94x0k5092zgfxzfr4p5680gc0000gn/T/ipykernel_95024/382003188.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  nn2 = torch.load('nav_model.pth')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NavigationNet(\n",
       "  (control_net): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=16, out_features=2, bias=True)\n",
       "    (11): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39355936  5.31137938]\n",
      " [-4.41325133  0.62367312]\n",
      " [-5.38880888  3.06664047]\n",
      " ...\n",
      " [-4.09309931  4.82532324]\n",
      " [ 2.21782178 -2.35133938]\n",
      " [-7.11944194  3.37429102]]\n",
      "(512, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_bot_positions(batch_size):\n",
    "    # Precompute candidate x values\n",
    "    all_x = np.linspace(-7.5, 7.6, 10000)\n",
    "    exclude_x = np.concatenate([\n",
    "        np.linspace(-2.6, -1.7, 900),\n",
    "        np.linspace(-0.8, 0.4, 1200),\n",
    "        np.linspace(1.5, 2.4, 900),\n",
    "        np.linspace(3.4, 4.6, 1200)\n",
    "    ])\n",
    "    # Using set difference and sorting to ensure reproducibility if needed\n",
    "    candidate_x = np.array(sorted(set(all_x) - set(exclude_x)))\n",
    "    \n",
    "    # Precompute candidate y values\n",
    "    all_y = np.linspace(-5.5, 5.6, 14000)\n",
    "    exclude_y = np.concatenate([\n",
    "        np.linspace(-1.5, 2.5, 1000),\n",
    "        np.linspace(-2.5, -5.6, 3100)\n",
    "    ])\n",
    "    candidate_y = np.array(sorted(set(all_y) - set(exclude_y)))\n",
    "    \n",
    "    # Sample batch_size positions for x and y independently\n",
    "    x_samples = np.random.choice(candidate_x, size=batch_size, replace=True)\n",
    "    y_samples = np.random.choice(candidate_y, size=batch_size, replace=True)\n",
    "    \n",
    "    # Combine the samples into an array of shape (batch_size, 2)\n",
    "    positions = np.stack((x_samples, y_samples), axis=1)\n",
    "    return positions\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    batch = gen_bot_positions(512)\n",
    "    print(batch)\n",
    "    print(batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Cannot squeeze non-singleton dims (vectorᶜ=512) from \u001b[92m(vectorᶜ=512)\u001b[0m \u001b[93mfloat64\u001b[0m \u001b[94m-0.063 ± 4.366\u001b[0m \u001b[37m(-7e+00...8e+00)\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 65\u001b[0m\n\u001b[0;32m     61\u001b[0m     goal_pos \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mstack([goal_x, goal_y], vec_dim)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m initial_pos, goal_pos\n\u001b[1;32m---> 65\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(test\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[21], line 41\u001b[0m, in \u001b[0;36mgenerate_batch\u001b[1;34m(batch_size, min_distance, max_distance)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Generate initial positions\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# initial_x = math.random_uniform(batch_dim, low=-8, high=-2.6)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# initial_y = math.random_uniform(batch_dim, low=-7, high=6)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# initial_pos = math.stack([initial_x, initial_y], vec_dim)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m positions \u001b[38;5;241m=\u001b[39m gen_bot_positions(batch_size)\n\u001b[1;32m---> 41\u001b[0m initial_pos \u001b[38;5;241m=\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Generate random displacement direction (angles)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m angle \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mrandom_uniform(batch_dim, low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi, high\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi)\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\phiml\\math\\_magic_ops.py:276\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(values, dim, expand_values, simplify, layout_non_matching, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MagicNotImplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one item in values must be Shapable but got types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mvalues]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m__stack__(values, dim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multi-dim stack\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m dim\u001b[38;5;241m.\u001b[39mvolume \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(values), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen passing multiple stack dims, their volume must equal len(values) but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\phiml\\math\\_tensors.py:494\u001b[0m, in \u001b[0;36mTensor.__stack__\u001b[1;34m(values, dim, **_kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layout_\u001b[38;5;241m.\u001b[39m__stack__(values, dim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stack_tensors\n\u001b[1;32m--> 494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstack_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\phiml\\math\\_ops.py:837\u001b[0m, in \u001b[0;36mstack_tensors\u001b[1;34m(values, dim)\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    836\u001b[0m values \u001b[38;5;241m=\u001b[39m [wrap(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[1;32m--> 837\u001b[0m values \u001b[38;5;241m=\u001b[39m [squeeze(v, dim) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[0;32m    838\u001b[0m values \u001b[38;5;241m=\u001b[39m cast_same(\u001b[38;5;241m*\u001b[39mvalues)\n\u001b[0;32m    839\u001b[0m \u001b[38;5;66;03m# --- sparse to dense ---\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\phiml\\math\\_ops.py:837\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    836\u001b[0m values \u001b[38;5;241m=\u001b[39m [wrap(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[1;32m--> 837\u001b[0m values \u001b[38;5;241m=\u001b[39m [\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[0;32m    838\u001b[0m values \u001b[38;5;241m=\u001b[39m cast_same(\u001b[38;5;241m*\u001b[39mvalues)\n\u001b[0;32m    839\u001b[0m \u001b[38;5;66;03m# --- sparse to dense ---\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\nav\\lib\\site-packages\\phiml\\math\\_magic_ops.py:555\u001b[0m, in \u001b[0;36msqueeze\u001b[1;34m(x, dims)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dims:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m--> 555\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dims\u001b[38;5;241m.\u001b[39mvolume \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot squeeze non-singleton dims \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x[{d: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims\u001b[38;5;241m.\u001b[39mnames}]\n",
      "\u001b[1;31mAssertionError\u001b[0m: Cannot squeeze non-singleton dims (vectorᶜ=512) from \u001b[92m(vectorᶜ=512)\u001b[0m \u001b[93mfloat64\u001b[0m \u001b[94m-0.063 ± 4.366\u001b[0m \u001b[37m(-7e+00...8e+00)\u001b[0m"
     ]
    }
   ],
   "source": [
    "from phi.torch import *\n",
    "from phi.flow import *\n",
    "\n",
    "def gen_bot_positions(batch_size):\n",
    "    # Precompute candidate x values\n",
    "    all_x = np.linspace(-7.5, 7.6, 10000)\n",
    "    exclude_x = np.concatenate([\n",
    "        np.linspace(-2.6, -1.7, 900),\n",
    "        np.linspace(-0.8, 0.4, 1200),\n",
    "        np.linspace(1.5, 2.4, 900),\n",
    "        np.linspace(3.4, 4.6, 1200)\n",
    "    ])\n",
    "    # Using set difference and sorting to ensure reproducibility if needed\n",
    "    candidate_x = np.array(sorted(set(all_x) - set(exclude_x)))\n",
    "    \n",
    "    # Precompute candidate y values\n",
    "    all_y = np.linspace(-5.5, 5.6, 14000)\n",
    "    exclude_y = np.concatenate([\n",
    "        np.linspace(-1.5, 2.5, 1000),\n",
    "        np.linspace(-2.5, -5.6, 3100)\n",
    "    ])\n",
    "    candidate_y = np.array(sorted(set(all_y) - set(exclude_y)))\n",
    "    \n",
    "    # Sample batch_size positions for x and y independently\n",
    "    x_samples = np.random.choice(candidate_x, size=batch_size, replace=True)\n",
    "    y_samples = np.random.choice(candidate_y, size=batch_size, replace=True)\n",
    "    \n",
    "    # Combine the samples into an array of shape (batch_size, 2)\n",
    "    positions = np.stack((x_samples, y_samples), axis=1)\n",
    "    return positions\n",
    "\n",
    "def generate_batch(batch_size, min_distance=0.5, max_distance=22):\n",
    "    batch_dim = batch(samples=batch_size)\n",
    "    vec_dim = channel(vector='x,y')\n",
    "    \n",
    "    # Generate initial positions\n",
    "    # initial_x = math.random_uniform(batch_dim, low=-8, high=-2.6)\n",
    "    # initial_y = math.random_uniform(batch_dim, low=-7, high=6)\n",
    "    # initial_pos = math.stack([initial_x, initial_y], vec_dim)\n",
    "    positions = gen_bot_positions(batch_size)\n",
    "    initial_pos = math.stack([positions[:, 0], positions[:, 1]] , vec_dim)\n",
    "    \n",
    "    # Generate random displacement direction (angles)\n",
    "    angle = math.random_uniform(batch_dim, low=-math.pi, high=math.pi)\n",
    "    \n",
    "    # Generate displacement magnitudes between [min_distance, max_distance]\n",
    "    distance = math.random_uniform(batch_dim, low=min_distance, high=max_distance)\n",
    "    \n",
    "    # Compute displacement components\n",
    "    dx = distance * math.cos(angle)\n",
    "    dy = distance * math.sin(angle)\n",
    "    \n",
    "    # Apply displacement to initial positions\n",
    "    goal_x = (initial_pos['x'] + dx)\n",
    "    goal_y = (initial_pos['y'] + dy)\n",
    "    \n",
    "    # Clamp goals to stay within bounds\n",
    "    goal_x = math.clip(goal_x, -7.9, 7.9)\n",
    "    goal_y = math.clip(goal_y, -5.9, 5.9)\n",
    "    \n",
    "    goal_pos = math.stack([goal_x, goal_y], vec_dim)\n",
    "    \n",
    "    return initial_pos, goal_pos\n",
    "\n",
    "test = generate_batch(512, min_distance=2.0, max_distance=10.0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial positions shape: (samplesᵇ=512, vectorᶜ=x,y)\n",
      "Goal positions shape: (samplesᵇ=512, vectorᶜ=x,y)\n"
     ]
    }
   ],
   "source": [
    "from phi.torch import *\n",
    "from phi.flow import *\n",
    "import numpy as np\n",
    "\n",
    "def gen_bot_positions(batch_size):\n",
    "    # Precompute candidate x values\n",
    "    all_x = np.linspace(-3, 5, 10000)\n",
    "    exclude_x = np.concatenate([\n",
    "        np.linspace(-2.8, -1.7, 900),\n",
    "        np.linspace(-1.2, 0.5, 1200),\n",
    "        np.linspace(1.5, 2.6, 900),\n",
    "        np.linspace(3.2, 4.6, 1200)\n",
    "    ])\n",
    "    candidate_x = np.array(sorted(set(all_x) - set(exclude_x)))\n",
    "    \n",
    "    # Precompute candidate y values\n",
    "    all_y = np.linspace(-7, 4, 14000)\n",
    "    # exclude_y = np.concatenate([\n",
    "    #     np.linspace(-1.5, 2.5, 1000),\n",
    "    #     np.linspace(-2.5, -5.6, 3100)\n",
    "    # ])\n",
    "    # candidate_y = np.array(sorted(set(all_y) - set(exclude_y)))\n",
    "    candidate_y = all_y\n",
    "    \n",
    "    # Sample batch_size positions for x and y independently\n",
    "    x_samples = np.random.choice(candidate_x, size=batch_size, replace=True)\n",
    "    y_samples = np.random.choice(candidate_y, size=batch_size, replace=True)\n",
    "    \n",
    "    # Combine the samples into an array of shape (batch_size, 2)\n",
    "    positions = np.stack((x_samples, y_samples), axis=1)\n",
    "    return positions\n",
    "\n",
    "def generate_batch(batch_size, min_distance=0.5, max_distance=22):\n",
    "    # Define channel dimensions for batch and vector components.\n",
    "    batch_dim = batch(samples=batch_size)\n",
    "    vec_dim = channel(vector='x,y')\n",
    "    \n",
    "    positions = gen_bot_positions(batch_size)\n",
    "    \n",
    "    initial_pos = math.tensor(positions, batch('samples'), channel(vector='x,y'))\n",
    "    \n",
    "    # Generate random displacement directions (angles)\n",
    "    angle = math.random_uniform(batch_dim, low=-math.pi, high=math.pi)\n",
    "    \n",
    "    # Generate displacement magnitudes between [min_distance, max_distance]\n",
    "    distance = math.random_uniform(batch_dim, low=min_distance, high=max_distance)\n",
    "    \n",
    "    # Compute displacement components using phi Flow math functions.\n",
    "    dx = distance * math.cos(angle)\n",
    "    dy = distance * math.sin(angle)\n",
    "    \n",
    "    # Apply displacement to initial positions.\n",
    "    goal_x = initial_pos['x'] + dx\n",
    "    goal_y = initial_pos['y'] + dy\n",
    "    \n",
    "    # Clamp goal positions to stay within defined bounds.\n",
    "    goal_x = math.clip(goal_x, -7.9, 7.9)\n",
    "    goal_y = math.clip(goal_y, -5.9, 5.9)\n",
    "    \n",
    "    # Stack the goal coordinates to create the goal position tensor.\n",
    "    goal_pos = math.stack([goal_x, goal_y], vec_dim)\n",
    "    \n",
    "    return initial_pos, goal_pos\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    initial_pos, goal_pos = generate_batch(512, min_distance=2.0, max_distance=10.0)\n",
    "    print(\"Initial positions shape:\", initial_pos.shape)\n",
    "    print(\"Goal positions shape:\", goal_pos.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(samplesᵇ=512, vectorᶜ=x,y) (samplesᵇ=512, vectorᶜ=x,y)\n"
     ]
    }
   ],
   "source": [
    "def generate_batch(batch_size, min_distance=0.5, max_distance=22):\n",
    "    batch_dim = batch(samples=batch_size)\n",
    "    vec_dim = channel(vector='x,y')\n",
    "    \n",
    "    # Generate initial positions\n",
    "    initial_x = math.random_uniform(batch_dim, low=-8, high=-2.6)\n",
    "    initial_y = math.random_uniform(batch_dim, low=-7, high=6)\n",
    "    initial_pos = math.stack([initial_x, initial_y], vec_dim)\n",
    "    \n",
    "    # Generate random displacement direction (angles)\n",
    "    angle = math.random_uniform(batch_dim, low=-math.pi, high=math.pi)\n",
    "    \n",
    "    # Generate displacement magnitudes between [min_distance, max_distance]\n",
    "    distance = math.random_uniform(batch_dim, low=min_distance, high=max_distance)\n",
    "    \n",
    "    # Compute displacement components\n",
    "    dx = distance * math.cos(angle)\n",
    "    dy = distance * math.sin(angle)\n",
    "    \n",
    "    # Apply displacement to initial positions\n",
    "    goal_x = (initial_x + dx)\n",
    "    goal_y = (initial_y + dy)\n",
    "    \n",
    "    # Clamp goals to stay within bounds\n",
    "    goal_x = math.clip(goal_x, -7.9, 7.9)\n",
    "    goal_y = math.clip(goal_y, -5.9, 5.9)\n",
    "    \n",
    "    goal_pos = math.stack([goal_x, goal_y], vec_dim)\n",
    "    \n",
    "    return initial_pos, goal_pos\n",
    "\n",
    "st, go = generate_batch(512, min_distance=2.0, max_distance=10.0)\n",
    "print(st.shape, go.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

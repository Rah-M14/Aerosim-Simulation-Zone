{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NavigationNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.control_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(6, 16),  # Input: [bot_x, bot_y, goal_x, goal_y, world_theta, relative_theta]\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 2), # Output: [L, delta_theta]\n",
    "            torch.nn.Tanh()  # Tanh activation for bounded output\n",
    "        )\n",
    "        \n",
    "    def forward(self, current_state):\n",
    "        return self.control_net(current_state)\n",
    "\n",
    "# net = torch.load(r\"nav_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), \"ext_nav_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.bias\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "linear_head.0.weight\n",
      "linear_head.0.bias\n",
      "linear_head.2.weight\n",
      "linear_head.2.bias\n",
      "linear_head.4.weight\n",
      "linear_head.4.bias\n",
      "angular_head.0.weight\n",
      "angular_head.0.bias\n",
      "angular_head.2.weight\n",
      "angular_head.2.bias\n",
      "angular_head.4.weight\n",
      "angular_head.4.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahat\\AppData\\Local\\Temp\\ipykernel_15460\\2239639905.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1 = torch.load(r\"F:\\Aerosim-Simulation-Zone\\Try\\mod_act_nav.pth\")\n"
     ]
    }
   ],
   "source": [
    "model1 = torch.load(r\"F:\\Aerosim-Simulation-Zone\\Try\\mod_act_nav.pth\")\n",
    "for key in model1.state_dict():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_orig_mod.log_std\n",
      "_orig_mod.mlp_extractor.policy_net.0.weight\n",
      "_orig_mod.mlp_extractor.policy_net.0.bias\n",
      "_orig_mod.mlp_extractor.policy_net.2.weight\n",
      "_orig_mod.mlp_extractor.policy_net.2.bias\n",
      "_orig_mod.mlp_extractor.policy_net.4.weight\n",
      "_orig_mod.mlp_extractor.policy_net.4.bias\n",
      "_orig_mod.mlp_extractor.policy_net.6.weight\n",
      "_orig_mod.mlp_extractor.policy_net.6.bias\n",
      "_orig_mod.action_net.weight\n",
      "_orig_mod.action_net.bias\n",
      "_orig_mod.value_net.weight\n",
      "_orig_mod.value_net.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahat\\AppData\\Local\\Temp\\ipykernel_37540\\583094147.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2 = torch.load(r\"F:\\Aerosim-Simulation-Zone\\TEST_FILES\\New_final_bc_policy.pth\")\n"
     ]
    }
   ],
   "source": [
    "model2 = torch.load(r\"F:\\Aerosim-Simulation-Zone\\TEST_FILES\\New_final_bc_policy.pth\")\n",
    "\n",
    "for key,_ in model2['model_state_dict'].items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahat\\AppData\\Local\\Temp\\ipykernel_37540\\316882640.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1 = torch.load(\"Noise_Nav_Model.pth\")\n",
      "C:\\Users\\rahat\\AppData\\Local\\Temp\\ipykernel_37540\\316882640.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2 = torch.load(r\"F:\\Aerosim-Simulation-Zone\\TEST_FILES\\final_bc_policy.pth\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NavigationNet' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Transfer weights and remove '_orig_mod' prefix\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m old_key, new_key \u001b[38;5;129;01min\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 23\u001b[0m     new_state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mold_key\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     24\u001b[0m     new_state_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.bias\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model1[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.bias\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Save the updated model\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NavigationNet' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Create a mapping dictionary for the networks\n",
    "mapping = {\n",
    "    'control_net.0': 'mlp_extractor.policy_net.0',\n",
    "    'control_net.2': 'mlp_extractor.policy_net.2',\n",
    "    'control_net.4': 'mlp_extractor.policy_net.4',\n",
    "    'control_net.6': 'mlp_extractor.policy_net.6',\n",
    "    'control_net.8': 'mlp_extractor.policy_net.8',\n",
    "    'control_net.10': 'action_net.0'\n",
    "}\n",
    "\n",
    "# Load both models\n",
    "model1 = torch.load(\"Noise_Nav_Model.pth\")\n",
    "model2 = torch.load(r\"F:\\Aerosim-Simulation-Zone\\TEST_FILES\\final_bc_policy.pth\")\n",
    "\n",
    "# Create new state dict for model2\n",
    "new_state_dict = {}\n",
    "\n",
    "# Handle log_std - initialize it with small negative values for stable training\n",
    "new_state_dict['log_std'] = torch.ones(model2['model_state_dict']['_orig_mod.log_std'].shape) * -2.0\n",
    "\n",
    "# Transfer weights and remove '_orig_mod' prefix\n",
    "for old_key, new_key in mapping.items():\n",
    "    new_state_dict[f'{new_key}.weight'] = model1[f'{old_key}.weight']\n",
    "    new_state_dict[f'{new_key}.bias'] = model1[f'{old_key}.bias']\n",
    "\n",
    "# Save the updated model\n",
    "torch.save({'model_state_dict': new_state_dict}, \"updated_policy.pth\")\n",
    "\n",
    "model3 = torch.load(\"updated_policy.pth\")\n",
    "\n",
    "for key,_ in model3['model_state_dict'].items():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated policy saved at updated_policy.pth\n",
      "Mapped policy keys:\n",
      "log_std\n",
      "mlp_extractor.policy_net.0.weight\n",
      "mlp_extractor.policy_net.0.bias\n",
      "mlp_extractor.policy_net.2.weight\n",
      "mlp_extractor.policy_net.2.bias\n",
      "mlp_extractor.policy_net.4.weight\n",
      "mlp_extractor.policy_net.4.bias\n",
      "mlp_extractor.policy_net.6.weight\n",
      "mlp_extractor.policy_net.6.bias\n",
      "mlp_extractor.policy_net.8.weight\n",
      "mlp_extractor.policy_net.8.bias\n",
      "action_net.0.weight\n",
      "action_net.0.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahat\\AppData\\Local\\Temp\\ipykernel_37540\\1552629445.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1 = torch.load(policy, map_location=torch.device(\"cpu\"))\n",
      "C:\\Users\\rahat\\AppData\\Local\\Temp\\ipykernel_37540\\1552629445.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2 = torch.load(ref_policy, map_location=torch.device(\"cpu\"))\n",
      "C:\\Users\\rahat\\AppData\\Local\\Temp\\ipykernel_37540\\1552629445.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  updated_policy = torch.load(updated_policy_path, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "\n",
    "def load_mapped_policy(policy, ref_policy):\n",
    "    \"\"\"\n",
    "    Loads an input policy provided via args, obtains its state dictionary,\n",
    "    maps its weights to the target architecture, and returns the updated policy.\n",
    "    \n",
    "    Mapping:\n",
    "        'control_net.0'   -> 'mlp_extractor.policy_net.0'\n",
    "        'control_net.2'   -> 'mlp_extractor.policy_net.2'\n",
    "        'control_net.4'   -> 'mlp_extractor.policy_net.4'\n",
    "        'control_net.6'   -> 'mlp_extractor.policy_net.6'\n",
    "        'control_net.8'   -> 'mlp_extractor.policy_net.8'\n",
    "        'control_net.10'  -> 'action_net.0'\n",
    "    \n",
    "    Additionally, the log_std parameter is initialized with a small negative value.\n",
    "    \"\"\"\n",
    "    # Define the mapping.\n",
    "    mapping = {\n",
    "        'control_net.0': 'mlp_extractor.policy_net.0',\n",
    "        'control_net.2': 'mlp_extractor.policy_net.2',\n",
    "        'control_net.4': 'mlp_extractor.policy_net.4',\n",
    "        'control_net.6': 'mlp_extractor.policy_net.6',\n",
    "        'control_net.8': 'mlp_extractor.policy_net.8',\n",
    "        'control_net.10': 'action_net.0'\n",
    "    }\n",
    "    \n",
    "    # Load the input policy.\n",
    "    model1 = torch.load(policy, map_location=torch.device(\"cpu\"))\n",
    "    model2 = torch.load(ref_policy, map_location=torch.device(\"cpu\"))\n",
    "    \n",
    "    # Get the state dict from the model.\n",
    "    # If model1 is a PyTorch nn.Module, call state_dict(). Otherwise, assume it is already a dict.\n",
    "    if hasattr(model1, 'state_dict'):\n",
    "        source_state = model1.state_dict()\n",
    "    else:\n",
    "        source_state = model1\n",
    "\n",
    "    new_state_dict = {}\n",
    "    \n",
    "    # Initialize log_std with a small negative value.\n",
    "    # if '_orig_mod.log_std' in source_state:\n",
    "    #     log_std_shape = source_state['_orig_mod.log_std'].shape\n",
    "    # else:\n",
    "    #     # As a fallback, use the shape of one of the layers.\n",
    "    #     log_std_shape = source_state['control_net.0.weight'].shape\n",
    "    # new_state_dict['log_std'] = torch.ones(log_std_shape) * -2.0\n",
    "    new_state_dict['log_std'] = torch.ones(model2['model_state_dict']['_orig_mod.log_std'].shape) * -2.0\n",
    "\n",
    "    # Transfer and remap weights.\n",
    "    for old_key, new_key in mapping.items():\n",
    "        new_state_dict[f'{new_key}.weight'] = source_state[f'{old_key}.weight']\n",
    "        new_state_dict[f'{new_key}.bias'] = source_state[f'{old_key}.bias']\n",
    "\n",
    "    # Save the updated policy.\n",
    "    updated_policy_path = \"updated_policy.pth\"\n",
    "    torch.save({'model_state_dict': new_state_dict}, updated_policy_path)\n",
    "    print(f\"Updated policy saved at {updated_policy_path}\")\n",
    "    \n",
    "    # Optionally load it back for further usage.\n",
    "    updated_policy = torch.load(updated_policy_path, map_location=torch.device(\"cpu\"))\n",
    "    print(\"Mapped policy keys:\")\n",
    "    for key in updated_policy['model_state_dict'].keys():\n",
    "        print(key)\n",
    "    \n",
    "    return updated_policy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--policy', type=str, required=True, help=\"Path to input policy (.pth) file.\")\n",
    "    # args = parser.parse_args()\n",
    "    policy = \"Noise_Nav_Model.pth\"\n",
    "    ref_policy = r\"F:\\Aerosim-Simulation-Zone\\TEST_FILES\\New_final_bc_policy.pth\"\n",
    "    load_mapped_policy(policy, ref_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lidar_points(binary_img, current_pos, world_limits, num_rays=360, max_range=4.0):\n",
    "    \"\"\"\n",
    "    Get LiDAR first contact points in world coordinates using vectorized operations\n",
    "    Args:\n",
    "        binary_img: Binary image where 0 is obstacle, 1 is free space\n",
    "        current_pos: (x,y) position of the sensor in world coordinates (a 1D array or list of 2 values)\n",
    "        world_limits: Array of [[min_x, max_x], [min_y, max_y]] world boundaries\n",
    "        num_rays: Number of rays to cast (default 360 for 1-degree resolution)\n",
    "        max_range: Maximum range of the sensor in world units\n",
    "    Returns:\n",
    "        contact_points: Array of shape (num_rays, 2) with (x,y) coordinates relative to sensor position,\n",
    "                        zeros for rays that don't hit anything\n",
    "        lidar_dists: Array of shape (num_rays,) with the distance to the contact for each ray\n",
    "    \"\"\"\n",
    "    height, width = binary_img.shape\n",
    "\n",
    "    # Calculate transformation factors from world to image.\n",
    "    world_width = world_limits[0][1] - world_limits[0][0]\n",
    "    world_height = world_limits[1][1] - world_limits[1][0]\n",
    "    scale_x = width / world_width\n",
    "    scale_y = height / world_height\n",
    "\n",
    "    # Convert world position to image coordinates.\n",
    "    # Use the lower bounds: world_limits[0][0] for x and world_limits[1][0] for y.\n",
    "    img_x = int(round((current_pos[0] - world_limits[0][0]) * scale_x))\n",
    "    img_y = height - int(round((current_pos[1] - world_limits[1][0]) * scale_y))\n",
    "\n",
    "    # Convert max_range to pixels.\n",
    "    max_range_px = int(max_range * min(scale_x, scale_y))\n",
    "\n",
    "    # Generate all angles at once.\n",
    "    angles = np.linspace(0, 2 * np.pi, num_rays, endpoint=False)\n",
    "\n",
    "    # Generate direction vectors for all angles.\n",
    "    directions = np.stack([np.cos(angles), -np.sin(angles)], axis=0)  # Shape: (2, num_rays)\n",
    "\n",
    "    # Generate all ray lengths at once.\n",
    "    ray_lengths = np.arange(1, max_range_px)  # Shape: (max_range_px-1,)\n",
    "\n",
    "    # Calculate all possible points for all rays using broadcasting.\n",
    "    ray_points = ray_lengths[:, np.newaxis, np.newaxis] * directions[np.newaxis, :, :]\n",
    "    ray_points = np.transpose(ray_points, (0, 2, 1))  # Reshape to (max_range_px-1, num_rays, 2)\n",
    "\n",
    "    # Add sensor position to all points.\n",
    "    ray_points_x = ray_points[..., 0] + img_x  # (max_range_px-1, num_rays)\n",
    "    ray_points_y = ray_points[..., 1] + img_y\n",
    "\n",
    "    # Convert to integer coordinates.\n",
    "    ray_points_x = ray_points_x.astype(np.int32)\n",
    "    ray_points_y = ray_points_y.astype(np.int32)\n",
    "\n",
    "    # Create masks for valid points.\n",
    "    valid_x = (ray_points_x >= 0) & (ray_points_x < width)\n",
    "    valid_y = (ray_points_y >= 0) & (ray_points_y < height)\n",
    "    valid_points = valid_x & valid_y\n",
    "\n",
    "    # Initialize arrays to store contact points and distances.\n",
    "    contact_points = np.zeros((num_rays, 2))\n",
    "    lidar_dists = np.ones(num_rays) * np.inf\n",
    "\n",
    "    # Find first contact point for each ray.\n",
    "    for ray_idx in range(num_rays):\n",
    "        valid_ray_points = valid_points[:, ray_idx]\n",
    "        if not np.any(valid_ray_points):\n",
    "            continue\n",
    "\n",
    "        ray_x = ray_points_x[valid_ray_points, ray_idx]\n",
    "        ray_y = ray_points_y[valid_ray_points, ray_idx]\n",
    "\n",
    "        # Check for obstacles along the ray.\n",
    "        ray_values = binary_img[ray_y, ray_x]\n",
    "        obstacle_indices = np.where(ray_values == 0)[0]\n",
    "\n",
    "        if len(obstacle_indices) > 0:\n",
    "            # Get first contact point.\n",
    "            first_contact_idx = obstacle_indices[0]\n",
    "            px = ray_x[first_contact_idx]\n",
    "            py = ray_y[first_contact_idx]\n",
    "\n",
    "            # Convert back to world coordinates.\n",
    "            # Adding a 0.5 offset so we convert from pixel centers.\n",
    "            world_x = ((px + 0.5) / scale_x) + world_limits[0][0]\n",
    "            world_y = world_limits[1][0] + ((height - (py + 0.5)) / scale_y)\n",
    "\n",
    "            # Calculate relative coordinates from the sensor's current position.\n",
    "            rel_x = world_x - current_pos[0]\n",
    "            rel_y = world_y - current_pos[1]\n",
    "            dist = np.sqrt(rel_x**2 + rel_y**2)\n",
    "\n",
    "            if dist <= max_range:\n",
    "                contact_points[ray_idx] = [rel_x, rel_y]\n",
    "                lidar_dists[ray_idx] = dist\n",
    "\n",
    "    return contact_points, lidar_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_actions(lidar_points, proposed_action, n, debug=False):\n",
    "    \"\"\"\n",
    "    Adjust the proposed (L, θ) action to avoid collisions and move into the closest free space.\n",
    "    \n",
    "    This function takes the LiDAR scan data (batch, 360) and a proposed action (batch, 2) provided\n",
    "    in normalized units (both values between -1 and 1). The linear component (L) is interpreted as \n",
    "    L * max_step_size, and the angular component (θ) as θ * max_theta (in radians), where we assume:\n",
    "        max_step_size = 1.0  (agent's maximum displacement per step)\n",
    "        max_theta = π       (so that a normalized value of ±1 corresponds to ±π radians)\n",
    "    \n",
    "    For each sample in the batch, the function examines the LiDAR readings in a safety window of ±n \n",
    "    degrees around the current proposed angle (converted to degrees) to determine if the proposed \n",
    "    linear displacement would lead to a collision (i.e. if an obstacle is closer than the intended move).\n",
    "    \n",
    "    If a collision is predicted, the function searches over candidate angles (in a broader range, e.g. ±30°)\n",
    "    for the direction closest to the original that is free (i.e. whose window has a higher obstacle distance).\n",
    "    It then sets the new linear action to be the minimum of the original and (the candidate's safe distance \n",
    "    multiplied by a margin) and returns the adjusted action in normalized units.\n",
    "    \n",
    "    Args:\n",
    "        lidar_points (np.ndarray): Array of shape (batch, 360) with LiDAR distances.\n",
    "        proposed_action (np.ndarray): Array of shape (batch,2) with normalized action values in [-1,1]\n",
    "                                        for [linear, angular] displacement.\n",
    "        n (int): Safety margin in degrees for collision checking (±n).\n",
    "        debug (bool): If True, prints debug statements.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Adjusted action of shape (batch, 2) with normalized values in [-1,1].\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        # print(\"DEBUG: Entering adjust_actions\")\n",
    "        print(f\"DEBUG: Initial proposed_action: {proposed_action}\")\n",
    "\n",
    "    # If the proposed_action is 1-dimensional, expand its dimensions.\n",
    "    was_single_action = False\n",
    "    if proposed_action.ndim == 1:\n",
    "        # if debug:\n",
    "            # print(\"DEBUG: Detected 1D proposed_action. Expanding dimensions.\")\n",
    "        proposed_action = np.expand_dims(proposed_action, axis=0)\n",
    "        was_single_action = True\n",
    "    # if debug:\n",
    "        # print(f\"DEBUG: was_single_action: {was_single_action}\")\n",
    "        # print(f\"DEBUG: proposed_action shape after potential expansion: {proposed_action.shape}\")\n",
    "\n",
    "    # Environment constants.\n",
    "    max_step_size = 1.0  # Actual maximum displacement (units)\n",
    "    max_theta = np.pi    # Maximum angular change in radians (normalized value ±1)\n",
    "    # if debug:\n",
    "        # print(f\"DEBUG: max_step_size = {max_step_size}, max_theta = {max_theta}\")\n",
    "\n",
    "    batch_size = proposed_action.shape[0]\n",
    "    new_action = proposed_action.copy()\n",
    "    # if debug:\n",
    "        # print(f\"DEBUG: Batch size: {batch_size}\")\n",
    "        # print(f\"DEBUG: new_action initialized as: {new_action}\")\n",
    "\n",
    "    # Convert normalized actions to actual environment values.\n",
    "    # Actual linear displacement (taking absolute value to assess distance).\n",
    "    actual_L = np.abs(proposed_action[:, 0]) * max_step_size\n",
    "    if debug:\n",
    "        print(f\"DEBUG: Computed actual_L: {actual_L}\")\n",
    "\n",
    "    # Actual angular displacement in radians.\n",
    "    actual_theta_rad = proposed_action[:, 1] * max_theta  \n",
    "    # if debug:\n",
    "        # print(f\"DEBUG: Computed actual_theta_rad: {actual_theta_rad}\")\n",
    "\n",
    "    # Convert angle to degrees (0-360).\n",
    "    actual_theta_deg = (np.rad2deg(actual_theta_rad)) % 360\n",
    "    if debug:\n",
    "        print(f\"DEBUG: Computed actual_theta_deg: {actual_theta_deg}\")\n",
    "\n",
    "    # Expand lidar_points to include batch dimension if needed.\n",
    "    lidar_points = lidar_points[np.newaxis, :]\n",
    "    # if debug:\n",
    "        # print(f\"DEBUG: lidar_points shape after adding new axis: {lidar_points.shape}\")\n",
    "\n",
    "    # Loop over each sample (usually batch size is 1).\n",
    "    for i in range(batch_size):\n",
    "        if debug:\n",
    "            print(f\"DEBUG: Processing sample {i}\")\n",
    "        # Define a narrow safety window ±n degrees around the proposed angle.\n",
    "        window_offsets = np.arange(-n, n + 1)\n",
    "        if debug:\n",
    "            print(f\"DEBUG: window_offsets: {window_offsets}\")\n",
    "\n",
    "        proposal_angle = np.round(actual_theta_deg[i]).astype(int)  # integer version of proposed angle.\n",
    "        if debug:\n",
    "            print(f\"DEBUG: Sample {i} proposal_angle: {proposal_angle}\")\n",
    "\n",
    "        indices = (proposal_angle + window_offsets) % 360\n",
    "        if debug:\n",
    "            print(f\"DEBUG: Sample {i} safety window indices: {indices}\")\n",
    "\n",
    "        readings = lidar_points[i, indices]\n",
    "        if debug:\n",
    "            print(f\"DEBUG: Sample {i} LiDAR readings in safety window: {readings}\")\n",
    "            # print(f\"DEBUG: Sample {i} Proposed linear displacement (actual_L): {actual_L[i]}\")\n",
    "\n",
    "        # The proposed action would be unsafe if any reading in the safety window\n",
    "        # is less than or equal to the intended displacement.\n",
    "        if (readings <= max_step_size*1.5).any():\n",
    "            if debug:\n",
    "                print(f\"DEBUG: Sample {i}: Proposed action is unsafe.\")\n",
    "            # Search for candidate angles in a broader range (e.g. ±n degrees) to find a free path.\n",
    "            candidate_range = np.arange(-n, n+1)\n",
    "            candidate_angles = (proposal_angle + candidate_range) % 360\n",
    "            if debug:\n",
    "                print(f\"DEBUG: Sample {i}: candidate_range: {candidate_range}\")\n",
    "                print(f\"DEBUG: Sample {i}: candidate_angles: {candidate_angles}\")\n",
    "            best_candidate = None\n",
    "            best_diff = 360  # Initialize with maximum possible angular difference.\n",
    "            candidate_safe_distance = 0\n",
    "            \n",
    "            # For each candidate, evaluate its safety using a narrow window (±n).\n",
    "            for cand in candidate_angles:\n",
    "                cand_indices = (cand + window_offsets) % 360\n",
    "                cand_readings = lidar_points[i, cand_indices]\n",
    "                cand_safe_dist = np.min(cand_readings)\n",
    "                diff_angle = min(abs(cand - actual_theta_deg[i]), 360 - abs(cand - actual_theta_deg[i]))\n",
    "                if debug:\n",
    "                    print(f\"DEBUG: Sample {i}: Evaluating candidate angle {cand}:\")\n",
    "                    print(f\"       cand_indices: {cand_indices}\")\n",
    "                    print(f\"       cand_readings: {cand_readings}\")\n",
    "                    print(f\"       cand_safe_dist: {cand_safe_dist}, diff_angle: {diff_angle}\")\n",
    "                # Choose candidate if it has some free space and is closer in angle to the original.\n",
    "                if (cand_safe_dist > 0 and cand_safe_dist >= (max_step_size - 1e-2)) and diff_angle < best_diff:\n",
    "                    best_diff = diff_angle\n",
    "                    best_candidate = cand\n",
    "                    candidate_safe_distance = cand_safe_dist\n",
    "                    if debug:\n",
    "                        print(f\"DEBUG: Sample {i}: New best candidate found: {cand} with diff {best_diff} and safe distance {cand_safe_dist}\")\n",
    "\n",
    "            # If no candidate is found, fallback by reducing the linear displacement.\n",
    "            if best_candidate is None:\n",
    "                new_actual_L = actual_L[i] * 0.5\n",
    "                new_actual_theta_deg = actual_theta_deg[i]\n",
    "                if debug:\n",
    "                    print(f\"DEBUG: Sample {i}: No free candidate found; halving L to {new_actual_L}.\")\n",
    "            else:\n",
    "                new_actual_theta_deg = best_candidate\n",
    "                margin = 0.9  # Safety margin: do not use the full available free distance.\n",
    "                new_actual_L = min(actual_L[i], candidate_safe_distance * margin)\n",
    "                if debug:\n",
    "                    print(f\"DEBUG: Sample {i}: Candidate angle {best_candidate}° selected with safe distance {candidate_safe_distance}.\")\n",
    "                    print(f\"DEBUG: Sample {i}: New linear displacement set to {new_actual_L}.\")\n",
    "            \n",
    "            # Update the action in normalized units.\n",
    "            new_action[i, 0] = np.clip(new_actual_L / max_step_size, -1, 1)\n",
    "            new_theta_rad = np.deg2rad(new_actual_theta_deg)\n",
    "            new_action[i, 1] = np.clip(new_theta_rad / max_theta, -1, 1)\n",
    "            if debug:\n",
    "                print(f\"DEBUG: Sample {i}: Updated action: linear = {new_action[i, 0]}, angular (normalized) = {new_action[i, 1]}\")\n",
    "        elif (readings <= max_step_size*3).any():\n",
    "            new_action[i, 0] = actual_L/2\n",
    "            if debug:\n",
    "                print(f\"DEBUG: Sample {i}: Proposed action is safe but L is halved {new_action[i, 0]}\")\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"DEBUG: Sample {i}: Proposed action is safe.\")\n",
    "                pass\n",
    "\n",
    "    # If the input was a single action (1D), return a 1D array.\n",
    "    if was_single_action:\n",
    "        if debug:\n",
    "            print(f\"DEBUG: Returning single action: {new_action[0]}\")\n",
    "        return new_action[0]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"DEBUG: Returning batch of actions: {new_action}\")\n",
    "        print(\"DEBUG: Exiting adjust_actions\")\n",
    "    return new_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(net, initial_pos, goal_pos, bin_img, max_steps=12):\n",
    "    \"\"\"\n",
    "    Simulate and plot the trajectory of the agent.\n",
    "    \n",
    "    At each step:\n",
    "      - Obtain LiDAR readings (using get_lidar_points).\n",
    "      - Compute network outputs and adjust actions if needed.\n",
    "      - Update the agent's position and orientation.\n",
    "      - Save the agent position and LiDAR contact points.\n",
    "    \n",
    "    After the simulation, the function plots:\n",
    "      - The trajectory (path).\n",
    "      - The start and goal positions.\n",
    "      - The LiDAR contact points (in orange) for every step.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        current_pos = initial_pos.clone()\n",
    "        goal_pos = goal_pos.clone()\n",
    "        theta = torch.zeros_like(current_pos[0])\n",
    "        positions = []\n",
    "        lidar_steps = []  # List to keep LiDAR contacts (absolute positions) per step\n",
    "\n",
    "        for stp in range(max_steps):\n",
    "            # Convert current position to a numpy array (shape: [x,y]) before passing to get_lidar_points.\n",
    "            current_pos_np = current_pos.cpu().numpy()\n",
    "            lidar_pts, lidar_dists = get_lidar_points(\n",
    "                binary_img=bin_img,\n",
    "                current_pos=current_pos_np,\n",
    "                world_limits=np.array([[-10, 10], [-8, 8]])\n",
    "            )\n",
    "            # Compute absolute LiDAR contact positions by adding the sensor's current position.\n",
    "            abs_lidar_pts = lidar_pts + current_pos_np\n",
    "            lidar_steps.append(abs_lidar_pts)\n",
    "\n",
    "            delta_pos = goal_pos - current_pos\n",
    "            relative_theta = torch.atan2(delta_pos[1], delta_pos[0]) - theta\n",
    "            relative_theta = (relative_theta + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "            print(np.array([current_pos[0]/10, current_pos[1]/7]),\n",
    "                  np.array([goal_pos[0]/10, goal_pos[1]/7]),\n",
    "                  np.array([theta.item()/np.pi, relative_theta.item()/np.pi]))\n",
    "\n",
    "            net_input = torch.stack([\n",
    "                current_pos[0]/10, current_pos[1]/7,\n",
    "                goal_pos[0]/10, goal_pos[1]/7,\n",
    "                theta/np.pi, relative_theta/np.pi\n",
    "            ], dim=-1).unsqueeze(0)\n",
    "\n",
    "            controls = net(net_input)[0]\n",
    "            L = controls[0]\n",
    "            delta_theta = controls[1] * np.pi\n",
    "\n",
    "            # (Optional) adjust the control with the LiDAR-based action correction.\n",
    "            new_act = adjust_actions(lidar_dists, controls, n=5, debug=False)\n",
    "\n",
    "            # Update the heading angle.\n",
    "            theta = theta + delta_theta\n",
    "            theta = (theta + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "            # Calculate the movement vector.\n",
    "            movement = torch.stack([\n",
    "                L * torch.cos(theta),\n",
    "                L * torch.sin(theta)\n",
    "            ])\n",
    "            current_pos += movement\n",
    "            positions.append(current_pos.cpu().numpy().copy())\n",
    "            print(f\"Actions: {L.item(), delta_theta.item()/np.pi}\")\n",
    "\n",
    "            if torch.norm(delta_pos) < 0.1:\n",
    "                break\n",
    "\n",
    "        positions = np.array(positions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        # Plot the trajectory.\n",
    "        plt.plot(positions[:, 0], positions[:, 1], 'b-o', markersize=4, label='Path')\n",
    "        plt.scatter(positions[0, 0], positions[0, 1], c='green', s=200, marker='*', label='Start')\n",
    "        plt.scatter(goal_pos[0].item(), goal_pos[1].item(), c='red', s=200, marker='X', label='Goal')\n",
    "\n",
    "        # Plot LiDAR contacts for each step using the absolute LiDAR positions.\n",
    "        first = True\n",
    "        for lidar in lidar_steps:\n",
    "            if first:\n",
    "                plt.scatter(lidar[:, 0], lidar[:, 1],\n",
    "                            c='orange', s=5, alpha=0.5, label='LiDAR')\n",
    "                first = False\n",
    "            else:\n",
    "                plt.scatter(lidar[:, 0], lidar[:, 1],\n",
    "                            c='orange', s=5, alpha=0.5)\n",
    "        # Ensure the legend has only one 'LiDAR' label.\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        unique = dict(zip(labels, handles))\n",
    "        plt.legend(unique.values(), unique.keys())\n",
    "\n",
    "        plt.title(\"Navigation Trajectory with LiDAR Contacts\")\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.grid(True)\n",
    "        plt.axis('equal')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_trajectory_with_slider(net, initial_pos, goal_pos, bin_img, max_steps=12):\n",
    "    \"\"\"\n",
    "    Simulate the agent's trajectory and create an interactive animation using Plotly.\n",
    "    \n",
    "    The animation displays:\n",
    "      - The cumulative agent's path as a blue line.\n",
    "      - The start (green star) and goal (red X) markers.\n",
    "      - The LiDAR contact points (orange markers) at each simulation step.\n",
    "    \n",
    "    To avoid nbformat-related errors, we force Plotly to use the \"browser\" renderer.\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.io as pio\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Force the Plotly renderer to \"browser\" to avoid nbformat issues.\n",
    "    pio.renderers.default = \"browser\"\n",
    "    \n",
    "    # --- Simulation Phase: Collect trajectory and LiDAR data ---\n",
    "    with torch.no_grad():\n",
    "        current_pos = initial_pos.clone()\n",
    "        goal_pos = goal_pos.clone()\n",
    "        theta = torch.zeros_like(current_pos[0])\n",
    "        positions = []     # Store agent positions at each step\n",
    "        lidar_steps = []   # Store absolute LiDAR positions (per step)\n",
    "    \n",
    "        for stp in range(max_steps):\n",
    "            # Convert the current position to a NumPy array.\n",
    "            current_pos_np = current_pos.cpu().numpy()\n",
    "            lidar_pts, lidar_dists = get_lidar_points(\n",
    "                binary_img=bin_img,\n",
    "                current_pos=current_pos_np,\n",
    "                world_limits=np.array([[-10, 10], [-8, 8]])\n",
    "            )\n",
    "            # Absolute LiDAR positions: shift relative readings by the agent position.\n",
    "            abs_lidar_pts = lidar_pts + current_pos_np\n",
    "            lidar_steps.append(abs_lidar_pts)\n",
    "    \n",
    "            delta_pos = goal_pos - current_pos\n",
    "            relative_theta = torch.atan2(delta_pos[1], delta_pos[0]) - theta\n",
    "            relative_theta = (relative_theta + np.pi) % (2 * np.pi) - np.pi\n",
    "    \n",
    "            net_input = torch.stack([\n",
    "                current_pos[0] / 10, current_pos[1] / 7,\n",
    "                goal_pos[0] / 10, goal_pos[1] / 7,\n",
    "                theta / np.pi, relative_theta / np.pi\n",
    "            ], dim=-1).unsqueeze(0)\n",
    "    \n",
    "            controls = net(net_input)[0]\n",
    "            L = controls[0]\n",
    "            delta_theta = controls[1] * np.pi\n",
    "    \n",
    "            # (Optional) adjust the control using LiDAR-based correction.\n",
    "            new_act = adjust_actions(lidar_dists, controls, n=5, debug=False)\n",
    "    \n",
    "            # Update heading and position.\n",
    "            theta = theta + delta_theta\n",
    "            theta = (theta + np.pi) % (2 * np.pi) - np.pi\n",
    "            movement = torch.stack([L * torch.cos(theta), L * torch.sin(theta)])\n",
    "            current_pos += movement\n",
    "            positions.append(current_pos.cpu().numpy().copy())\n",
    "    \n",
    "            if torch.norm(delta_pos) < 0.1:\n",
    "                break\n",
    "    \n",
    "    positions = np.array(positions)\n",
    "    n_frames = len(positions)\n",
    "    \n",
    "    # Determine plot boundaries.\n",
    "    margin = 1.0\n",
    "    x_min = min(np.min(positions[:, 0]), goal_pos[0].item(), initial_pos[0].item()) - margin\n",
    "    x_max = max(np.max(positions[:, 0]), goal_pos[0].item(), initial_pos[0].item()) + margin\n",
    "    y_min = min(np.min(positions[:, 1]), goal_pos[1].item(), initial_pos[1].item()) - margin\n",
    "    y_max = max(np.max(positions[:, 1]), goal_pos[1].item(), initial_pos[1].item()) + margin\n",
    "    \n",
    "    # Define initial traces.\n",
    "    path_trace = go.Scatter(\n",
    "        x=positions[:1, 0],\n",
    "        y=positions[:1, 1],\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='blue'),\n",
    "        name='Path'\n",
    "    )\n",
    "    start_trace = go.Scatter(\n",
    "        x=[initial_pos[0].item()],\n",
    "        y=[initial_pos[1].item()],\n",
    "        mode='markers',\n",
    "        marker=dict(color='green', size=15, symbol='star'),\n",
    "        name='Start'\n",
    "    )\n",
    "    goal_trace = go.Scatter(\n",
    "        x=[goal_pos[0].item()],\n",
    "        y=[goal_pos[1].item()],\n",
    "        mode='markers',\n",
    "        marker=dict(color='red', size=15, symbol='x'),\n",
    "        name='Goal'\n",
    "    )\n",
    "    lidar_trace = go.Scatter(\n",
    "        x=lidar_steps[0][:, 0],\n",
    "        y=lidar_steps[0][:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(color='orange', size=5),\n",
    "        name='LiDAR'\n",
    "    )\n",
    "    \n",
    "    # Build animation frames.\n",
    "    frames = []\n",
    "    for i in range(n_frames):\n",
    "        frame_data = [\n",
    "            go.Scatter(\n",
    "                x=positions[:i+1, 0],\n",
    "                y=positions[:i+1, 1],\n",
    "                mode='lines+markers',\n",
    "                line=dict(color='blue')\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=[initial_pos[0].item()],\n",
    "                y=[initial_pos[1].item()],\n",
    "                mode='markers',\n",
    "                marker=dict(color='green', size=15, symbol='star')\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=[goal_pos[0].item()],\n",
    "                y=[goal_pos[1].item()],\n",
    "                mode='markers',\n",
    "                marker=dict(color='red', size=15, symbol='x')\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=lidar_steps[i][:, 0],\n",
    "                y=lidar_steps[i][:, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(color='orange', size=5)\n",
    "            )\n",
    "        ]\n",
    "        frames.append(go.Frame(data=frame_data, name=str(i)))\n",
    "    \n",
    "    # Create slider steps.\n",
    "    slider_steps = []\n",
    "    for i in range(n_frames):\n",
    "        slider_steps.append({\n",
    "            \"args\": [[str(i)],\n",
    "                     {\"frame\": {\"duration\": 500, \"redraw\": True},\n",
    "                      \"mode\": \"immediate\",\n",
    "                      \"transition\": {\"duration\": 0}}],\n",
    "            \"label\": str(i),\n",
    "            \"method\": \"animate\"\n",
    "        })\n",
    "    \n",
    "    # Construct the figure.\n",
    "    fig = go.Figure(\n",
    "        data=[path_trace, start_trace, goal_trace, lidar_trace],\n",
    "        layout=go.Layout(\n",
    "            title=\"Agent Trajectory with LiDAR Contacts\",\n",
    "            xaxis=dict(range=[x_min, x_max], title=\"X\"),\n",
    "            yaxis=dict(range=[y_min, y_max], title=\"Y\"),\n",
    "            updatemenus=[{\n",
    "                \"type\": \"buttons\",\n",
    "                \"buttons\": [{\n",
    "                    \"label\": \"Play\",\n",
    "                    \"method\": \"animate\",\n",
    "                    \"args\": [None, {\"frame\": {\"duration\": 500, \"redraw\": True},\n",
    "                                    \"fromcurrent\": True,\n",
    "                                    \"transition\": {\"duration\": 0}}],\n",
    "                }]\n",
    "            }],\n",
    "            sliders=[{\n",
    "                \"active\": 0,\n",
    "                \"steps\": slider_steps,\n",
    "                \"currentvalue\": {\"prefix\": \"Step: \"},\n",
    "            }]\n",
    "        ),\n",
    "        frames=frames\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahat\\AppData\\Local\\Temp\\ipykernel_38352\\3389537835.py:155: DeprecationWarning:\n",
      "\n",
      "Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"F:\\Aerosim-Simulation-Zone\\TEST_FILES\\New_WR_World.png\"\n",
    "img = np.array(Image.open(image_path).convert('L'))  # Convert to grayscale\n",
    "bin_img = (img > 128).astype(np.uint8)  # Threshold to create a binary map\n",
    "bin_img = cv2.resize(bin_img, (0,0), fx=0.25, fy=0.25)\n",
    "\n",
    "with torch.no_grad():\n",
    "    initial_torch = torch.tensor([5, -3], dtype=torch.float32)\n",
    "    goal_torch = torch.tensor([0, -3], dtype=torch.float32)\n",
    "    animate_trajectory_with_slider(net, initial_torch, goal_torch, bin_img, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahat\\AppData\\Local\\Temp\\ipykernel_33692\\19126482.py:2: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmin(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.3576e-14])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([-20.0, 10.0])\n",
    "torch.nn.functional.softmin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deg : [390 270], rad: [6.80678408 4.71238898], diff: [-8.37758041 -6.28318531], abs_diff: [8.37758041 6.28318531]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "deg = np.array([390, 270])\n",
    "rad = np.deg2rad(deg)\n",
    "diff = np.ones(deg.shape[0])*(-np.pi/2) - rad\n",
    "print(f\"deg : {deg}, rad: {rad}, diff: {diff}, abs_diff: {np.abs(diff)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  5. inf]\n",
      "[False False  True]\n",
      "[False  True  True]\n",
      "[False  True False]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([3, 5, np.inf])\n",
    "print(a)\n",
    "ind = np.isinf(a)\n",
    "print(ind)\n",
    "sfd = a > 3.2\n",
    "print(sfd)\n",
    "fidx = sfd & (~ind)\n",
    "print(fidx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.80350850198276"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.power(14,2) + np.power(18,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai: 24.064227395494576, a2: 24.6371851906254, a3: 25.21014298575622\n"
     ]
    }
   ],
   "source": [
    "a1 = np.rad2deg(0.42)\n",
    "a2 = np.rad2deg(0.43)\n",
    "a3 = np.rad2deg(0.44)\n",
    "print(f\"ai: {a1}, a2: {a2}, a3: {a3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True]\n",
      "[[ True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a9917c5480>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from LiDAR_Fast import *\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from Lidar_Model_Enc import *\n",
    "\n",
    "def get_safe_mask_1(lidar_dists, threshold, num_rays=360, window_size=5):\n",
    "    \n",
    "    scan = np.asarray(lidar_dists)\n",
    "    \n",
    "    if len(scan.shape) > 1 and scan.shape[1] != num_rays:\n",
    "        scan = scan[:, :num_rays]\n",
    "    \n",
    "    finite_mask = (scan > threshold)\n",
    "    return finite_mask\n",
    "\n",
    "image_path = r\"F:\\Aerosim-Simulation-Zone\\Try\\New_WR_World.png\"\n",
    "img = np.array(Image.open(image_path).convert('L'))\n",
    "binary_img = (img > 128).astype(np.uint8)\n",
    "binary_img = cv2.resize(binary_img, (0,0), fx=0.25, fy=0.25)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "c_pos = np.array([-3.25, 5.8])\n",
    "world_limits = np.array([[-10, 10], [-8, 8]])\n",
    "\n",
    "lp, ld = get_lidar_points(binary_img, c_pos, world_limits)\n",
    "ld_1 = ld[np.newaxis, :]\n",
    "\n",
    "mask_1 = get_safe_mask_1(ld, 3)\n",
    "mask_2 = get_safe_mask(ld_1, threshold=3, window_size=12)\n",
    "\n",
    "print(mask_1)\n",
    "print(mask_2)\n",
    "\n",
    "# plt.plot(mask[0], label='m1')\n",
    "plt.plot(mask_1, label='m1')\n",
    "plt.plot(mask_2.flatten(), label='m2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([200.])\n",
      "safe weights : torch.Size([1, 360])\n",
      "safe Probs : torch.Size([1, 360])\n",
      "theta safe: tensor([197.])\n",
      "confidence: 0.25\n",
      "Orifinal L: tensor([0.]), Original Theta: tensor([200.])\n",
      "final L: tensor([0.]), final theta: tensor([197.7500]), final theta JUST: tensor([197.])\n",
      "Difference Blended: tensor([-2.2500]), Difference: tensor([-3.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 3.4514]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dist_trmp = nn.Parameter(torch.tensor(1.0))\n",
    "distance_temp = torch.tensor(2)\n",
    "\n",
    "# raw_output = features(state)\n",
    "# L = torch.sigmoid(raw_output[:, 0])  # [0,1] magnitude\n",
    "# theta_raw = raw_output[:, 1] * torch.pi  # [-π, π]\n",
    "\n",
    "raw_output = torch.tensor([[0, 200/360]])\n",
    "L = raw_output[:, 0]  # [0,1] magnitude\n",
    "theta_raw = (raw_output[:, 1] * 2*torch.pi) % (2*np.pi)  # [-π, π]\n",
    "\n",
    "# Convert to angular bins [0, angular_res)\n",
    "# theta_bin = np.rad2deg((theta_raw + torch.pi) % (2 * torch.pi) - torch.pi)\n",
    "theta_bin = np.rad2deg(theta_raw) % 360\n",
    "# --- Differentiable Nearest Safe Angle Selection ---\n",
    "print(theta_bin)\n",
    "# 1. Create distance matrix\n",
    "all_bins = torch.arange(360)\n",
    "bin_distances = torch.abs(all_bins.float() - theta_bin.unsqueeze(-1))\n",
    "# print(bin_distances[35:56])\n",
    "# print(f\"Bin distances shape: {bin_distances.shape}\")\n",
    "# plt.plot(bin_distances.flatten(), label='bin_distances')\n",
    "\n",
    "# 2. Apply safety mask and distance weighting\n",
    "safe_weights = F.softmax(-bin_distances / distance_temp, dim=-1)\n",
    "safe_weights = safe_weights * mask  # Zero out unsafe\n",
    "\n",
    "print(f\"safe weights : {safe_weights.shape}\")\n",
    "plt.plot(safe_weights.flatten(), label='weights')\n",
    "\n",
    "# 3. Differentiable nearest selection (Gumbel-softmax)\n",
    "safe_probs = F.gumbel_softmax(safe_weights.log(), tau=0.5, hard=True)\n",
    "\n",
    "print(f\"safe Probs : {safe_probs.shape}\")\n",
    "plt.plot(safe_probs.flatten(), label='probs')\n",
    "\n",
    "# 4. Get nearest safe angle\n",
    "nearest_bin = torch.argmax(safe_probs, dim=-1)\n",
    "theta_safe_deg = (nearest_bin.float())\n",
    "theta_safe = torch.deg2rad(theta_safe_deg)\n",
    "\n",
    "print(f\"theta safe: {np.rad2deg(theta_safe)}\")\n",
    "\n",
    "# 5. Blend with raw angle using distance confidence\n",
    "confidence = 1 / (1 + bin_distances.gather(-1, nearest_bin.unsqueeze(-1)).squeeze())\n",
    "final_theta = confidence * theta_raw + (1 - confidence) * theta_safe\n",
    "final_theta_just = theta_safe\n",
    "\n",
    "print(f\"confidence: {confidence}\")\n",
    "print(f\"Orifinal L: {L}, Original Theta: {np.rad2deg(theta_raw)}\")\n",
    "print(f\"final L: {L}, final theta: {np.rad2deg(final_theta)}, final theta JUST: {np.rad2deg(final_theta_just)}\")\n",
    "print(f\"Difference Blended: {np.rad2deg(((final_theta - theta_raw)))}, Difference: {np.rad2deg(((final_theta_just - theta_raw)))}\")\n",
    "plt.legend()\n",
    "\n",
    "torch.stack([L, final_theta], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-196.52452372987239"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.rad2deg(-3.43)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavigationNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Change input dimension from 6 to 12\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Linear(6, 16),  # New input: [bot_x, bot_y, goal_x, goal_y, world_theta, relative_theta]\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.linear_head = torch.nn.Sequential(\n",
    "            # torch.nn.Linear(64, 32),\n",
    "            # torch.nn.ReLU(),\n",
    "            # torch.nn.Linear(32, 16),\n",
    "            # torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()  # L will be in [0,1]\n",
    "        )\n",
    "        self.angular_head = torch.nn.Sequential(\n",
    "            # torch.nn.Linear(64, 32),\n",
    "            # torch.nn.ReLU(),\n",
    "            # torch.nn.Linear(32, 16),\n",
    "            # torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()  # delta_theta will be in [0,1]\n",
    "        )\n",
    "        # self.distance_temp = nn.Parameter(torch.tensor(2.0))\n",
    "        self.distance_temp = torch.tensor(2)\n",
    "\n",
    "    def forward(self, state, safe_angle_mask):\n",
    "        \"\"\"\n",
    "        state: [batch_size, 6] (input features)\n",
    "        safe_angle_mask: [batch_size, angular_res] (1=safe, 0=unsafe)\n",
    "        \"\"\"\n",
    "        # Base policy output\n",
    "        # print(f\"state shape: {state.shape}\")\n",
    "        raw_output = self.features(state)\n",
    "\n",
    "        L = self.linear_head(raw_output)  # [0,1] magnitude\n",
    "        theta_raw = (self.angular_head(raw_output) * (2 * torch.pi)) % (2 * torch.pi)  # [0, 2π]\n",
    "\n",
    "        # print(f\"MODEL: Theta_Raw: {torch.rad2deg(theta_raw)}\")\n",
    "        # Convert to angular bins [0, angular_res)\n",
    "        # theta_deg = torch.rad2deg(theta_raw + torch.pi) % 360\n",
    "        theta_deg = torch.rad2deg(theta_raw)\n",
    "        theta_bin = theta_deg.long()\n",
    "        # --- Differentiable Nearest Safe Angle Selection ---\n",
    "        # 1. Create distance matrix\n",
    "        all_bins = torch.arange(360, device=state.device)\n",
    "        bin_distances = torch.abs(all_bins.float() - theta_bin)\n",
    "        # print(f\"Bin distances: {bin_distances}\")\n",
    "        \n",
    "        # 2. Apply safety mask and distance weighting\n",
    "        safe_weights = F.softmax(-bin_distances / self.distance_temp, dim=-1)\n",
    "        # print(f\"Safe weights: {safe_weights}\")\n",
    "        # plt.plot(safe_weights.detach().cpu().numpy().flatten(), label='weights')\n",
    "        # plt.show()\n",
    "\n",
    "        safe_weights = safe_weights * safe_angle_mask  # Zero out unsafe\n",
    "        # print(f\"safe weights : {safe_weights.shape}\")\n",
    "\n",
    "        # 3. Differentiable nearest selection (Gumbel-softmax)\n",
    "        safe_probs = F.gumbel_softmax(safe_weights.log(), tau=0.5, hard=True)\n",
    "        # print(f\"safe Probs : {safe_probs.shape}\")\n",
    "\n",
    "        # 4. Get nearest safe angle\n",
    "        nearest_bin = torch.argmax(safe_probs, dim=-1)\n",
    "        theta_safe_deg = (nearest_bin.float())\n",
    "        theta_safe = torch.deg2rad(theta_safe_deg)\n",
    "        # print(f\"theta safe: {theta_safe.shape}\")\n",
    "        # print(f\"theta raw: {theta_raw.shape}\")\n",
    "    \n",
    "        # 5. Blend with raw angle using distance confidence\n",
    "        confidence = 1 / (1 + bin_distances.gather(-1, nearest_bin.unsqueeze(-1)).squeeze())\n",
    "        final_theta = confidence * theta_raw.squeeze() + (1 - confidence) * theta_safe\n",
    "        # print(f\"MODEL: Theta_Final: {torch.rad2deg(final_theta)}\")\n",
    "        # print(f\"MODEL: Difference: {torch.rad2deg(final_theta - theta_raw)}\")\n",
    "\n",
    "        # plt.plot(bin_distances[0].cpu().numpy().flatten())\n",
    "        # plt.plot(safe_weights.detach().cpu().numpy().flatten(), label='weights')\n",
    "        # plt.plot(safe_probs.detach().cpu().numpy().flatten(), label='probs')\n",
    "        # plt.plot(safe_weights.detach().cpu().numpy().flatten(), label='weights')\n",
    "        # plt.show()\n",
    "\n",
    "        # print(f\"confidence: {confidence.shape}\")\n",
    "        # print(f\"L final: {L.shape}\")\n",
    "        # print(f\"theta final: {final_theta.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "        return torch.stack([L.squeeze(-1), final_theta], dim=-1), torch.stack([L.squeeze(-1), theta_raw.squeeze(-1)], dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NavigationNet(\n",
      "  (features): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (9): ReLU()\n",
      "  )\n",
      "  (linear_head): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (angular_head): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "features.0.weight\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.bias\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "features.6.weight\n",
      "features.6.bias\n",
      "features.8.weight\n",
      "features.8.bias\n",
      "linear_head.0.weight\n",
      "linear_head.0.bias\n",
      "angular_head.0.weight\n",
      "angular_head.0.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = r\"F:\\Aerosim-Simulation-Zone\\Try\\FIGS_Bound\\checkpoint_epoch_500.pth\"\n",
    "model = torch.load(model_path, weights_only=False)\n",
    "\n",
    "print(model)\n",
    "for k in model.state_dict().keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
